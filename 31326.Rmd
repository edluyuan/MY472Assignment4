---
title: "Assignment 4"
author: "Candidate Number: 31326"
date: "2023-12-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# A Large Scale Analysis of Racial Profiling and Other Biases in Police Stop and Searches in the United Kingdom

*Note:* This Assignment discuss the research question stated in the **Option 1** of the paper. The github repo for this assignment can be found [here](https://github.com/edluyuan/MY472Assignment4). All codes, files, and data needed to replicate this assignment are included.

Word Count: 739 words (excl. References)

```{r environment, message=FALSE, include=FALSE, warning=FALSE, cache=TRUE}
library(tidyselect)
library(tidyr)
library(httr)
library(knitr)
library(jsonlite)
library(dplyr)
library(lubridate)
library(ggplot2)
library(scales)
library(MASS)
library(tools)
library(plotly)
library(lme4)
library(glmmTMB)
library(brms)
library(gridExtra)
library(forcats)
```

## Introduction

This UK study investigates potential biases in police stop-and-search authority based on reasonable suspicion. Analyzing over 1.4 million records from 2020 to 2023 across demographics like ethnicity, gender, and age, the research integrates census data to assess if certain groups are unfairly targeted, aiming to uncover bias in police actions.

## Data

The study analyzes 1.4 million UK police stop-and-search records from 2020 to 2023, using the police database [API](https://data.police.uk/api/). Despite some missing updates, the data captures essential variables like ethnicity and age, standardized for consistency. UK [census](https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates) integration allows for a population-relative examination of the stop-and-search practices.

```{r dataLoader, cache = TRUE, message=FALSE, include=FALSE, warning=FALSE}
# Data Loader
# Get Census Data
census <- read.csv("census.csv")
# Fetch the list of all police forces
forces_response <- GET("https://data.police.uk/api/forces")
forces_data <- fromJSON(rawToChar(forces_response$content))
# Extract the IDs of all forces
forces_ids <- forces_data$id
# Initialize an empty list to store data
all_forces_data <- list()
# many local forces have incomplete/did not update stop and search data for the year 2023
# we use data from 2020 to 2022 to ensure compeletness
# month index generator function
generate_monthly_dates <- function() {
  start_date <- as.Date("2020-12-01")
  end_date <- as.Date("2023-11-30")
  all_dates <- seq(start_date, end_date, by = "month")
  formatted_dates <- format(all_dates, "%Y-%m")
  return(formatted_dates)
}

# Generate monthly dates from 2019 to 2022
dates <- generate_monthly_dates()

# Initialize lists to store data, no data, and failed requests
all_forces_data <- list()
no_data_list <- list()
failed_requests_list <- list()

# Loop through each police force ID and date
for (force_id in forces_ids) {
  for (date in dates) {
    # Make an API request for stop and search data
    response <- GET(paste0("https://data.police.uk/api/stops-force?force=", force_id, "&date=", date))
    # Check the status of the response
    status <- http_status(response)

    # Create a unique key for each force and date
    key <- paste(force_id, date, sep = "_")

    # Check if the response is not an error
    if (status$category != "client_error" && status$category != "server_error") {
      # Try to parse the JSON response
      try({
        force_data <- fromJSON(rawToChar(response$content), flatten = TRUE)
        
        # If data is present, add it to the all_forces_data list
        if (length(force_data) > 0) {
          all_forces_data[[key]] <- force_data
        } else {
          # If no data, add NA to the no_data_list
          no_data_list[[key]] <- NA
        }
      }, silent = TRUE)
    } else {
      # If an error occurs, log the reason in failed_requests_list
      failed_requests_list[[key]] <- status$reason
    }
  }
}



## Function to get all unique columns from the list of data frames
get_all_columns <- function(data_list) {
  unique_columns <- unique(unlist(lapply(data_list, colnames)))
  return(unique_columns)
}

# Get all unique columns from the data
all_columns <- get_all_columns(all_forces_data)
# Function to standardize a data frame to have all columns
standardize_df <- function(df, all_columns) {
  # Add missing columns as NA
  missing_columns <- setdiff(all_columns, names(df))
  for (col in missing_columns) {
    df[[col]] <- NA
  }
  # Order columns to match all_columns
  df <- df[, all_columns]
  return(df)
}

# Function to check if an element is a valid, non-empty dataframe
is_valid_dataframe <- function(x) {
  is.data.frame(x) && nrow(x) > 0
}
# Apply standardization to each data frame
all_forces_data <- lapply(all_forces_data, function(x) {
  if (is_valid_dataframe(x)) {
    return(standardize_df(x, all_columns))
  } else {
    return(NULL)
  }
})
# Remove NULL elements
all_forces_data <- Filter(NROW, all_forces_data)

# Combine all standardized data frames
if (length(all_forces_data) > 0) {
  combined_data <- do.call(rbind, all_forces_data)

  # Extract file names and convert to title case
  file_names <- names(all_forces_data)
  formatted_names <- gsub("_.*", "", file_names)  # Remove everything after '_'
  formatted_names <- gsub("-", " ", formatted_names)  # Replace '-' with space
  formatted_names <- sapply(formatted_names, toTitleCase)  # Convert to title case

  # Calculate the number of rows for each file and repeat the names accordingly
  rows_per_file <- sapply(all_forces_data, nrow)
  combined_data$force <- rep(formatted_names, times = rows_per_file)
} else {
  combined_data <- data.frame(matrix(ncol = length(all_columns), nrow = 0))
  colnames(combined_data) <- all_columns
}
```

The study period saw over 85% of stop-and-search records successfully uploaded with no failures detected in data completeness.

```{r completenessCheck}
# completeness check
data.frame(
  # Key column: combines names from three different lists (all_forces_data, no_data_list, failed_requests_list)
  Key = c(names(all_forces_data), names(no_data_list), names(failed_requests_list)),

  # Status column: creates a vector indicating the status of each item in the Key column
  Status = c(rep("Data Collected", length(all_forces_data)),
             rep("No Data", length(no_data_list)),
             rep("Failed Request", length(failed_requests_list)))
) %>%
  # Count the number of occurrences of each unique status
  count(Status) %>%
  # Add a new column 'Percentage' showing the percentage of each status
  mutate(Percentage = n / sum(n) * 100) %>%
  # Select only the columns 'Status' and 'Percentage' for the final output
  select(Status, Percentage) 

```

more than 1.4 million search recorded

```{r}
combined_data %>% nrow()
```

```{r dataCleaning,  message=FALSE, include=FALSE, warning=FALSE}
# Remove non-person searches, NULLs, and irrelevant variables from 'combined_data'
combined_data <- combined_data %>%
  filter(
    involved_person = TRUE # Keep only rows where 'involved_person' is TRUE
    ) %>%
    select(
      # Exclude specified columns that are not needed for analysis
      !any_of(c("location", "operation", "operation_name", "self_defined_ethnicity", 
                "location.street.name", "outcome_object.id", "location.street.id"))
    )

# Recode 'officer_defined_ethnicity' from "Mixed" to "Other" and convert to a factor variable
combined_data$officer_defined_ethnicity <- ifelse(
  combined_data$officer_defined_ethnicity == "Mixed", 
  "Other", 
  combined_data$officer_defined_ethnicity
) %>% as.factor()

```

## Analysis

### Descriptive Statistics

#### Table 1: Proportion and count for search objects

```{r descriptivesObjects, echo=FALSE}
# Calculate and view the proportion of each 'object_of_search' category in 'combined_data'
combined_data %>%
  filter(!is.na(object_of_search)) %>% # Exclude rows with NA in 'object_of_search'
  count(object_of_search) %>% # Count the number of occurrences for each 'object_of_search'
  mutate(proportion = round(n*100 / sum(n), 2)) # Calculate the proportion and round to 2 decimal places
```

Controlled drugs (62.55%) and offensive weapons (14.67%) are the most commonly searched items

#### Table 2: Proportion and count for search ethnicity

```{r descriptivesRace, echo=FALSE}
# Calculate the proportion for each race
combined_data %>%
  filter(!is.na(officer_defined_ethnicity)) %>%
  count(officer_defined_ethnicity) %>%
  mutate(proportion = round(n*100 / sum(n), 2))  # View the result
```

Whites, who form the majority in England and Wales, accounted for 63.9% of stop-and-search counts.

```{r descriptivesOutcome, echo=FALSE}
# Calculate and visualize the proportion of outcomes by officer-defined ethnicity
combined_data %>%
  filter(!is.na(outcome), outcome != "", !is.na(officer_defined_ethnicity)) %>% # Filter out NA and empty 'outcome', and NA in 'officer_defined_ethnicity'
  group_by(officer_defined_ethnicity) %>% # Group data by 'officer_defined_ethnicity'
  count(outcome) %>% # Count the number of occurrences for each 'outcome'
  mutate(proportion = n / sum(n)) %>% # Calculate proportion within each group
  # Create a stacked bar plot
  ggplot( aes(x = officer_defined_ethnicity, y = proportion, fill = outcome)) +
    geom_bar(stat = "identity", position = position_stack(reverse = TRUE)) + # Stacked bar chart with reversed stack order
    labs(
      title = "Proportion of Outcomes by Officer-Defined Ethnicity",
      x = "Officer-Defined Ethnicity", y = "Proportion (%)",
      fill = "Outcome"
    ) +
    theme_minimal() + # Minimal theme for clarity
    scale_y_continuous(labels = scales::percent_format()) # Format y-axis as percentage
```

For all ethinicity nearly 75% of the searches leads to no further action, but there is notable in eye ball examination differences in arrest and penalty rate

### Identify Biases

To compare baselines, we need to understand the racial demographics by benchmark searches to the census-estimated population ratio

#### Ethinicity

```{r Race, echo=FALSE}
# Calculate and visualize demographic ratios in stop and searches over time
census_race <- gather(census, officer_defined_ethnicity, count, -age_range) %>%
  # Group by officer_defined_ethnicity and calculate total count for each ethnicity
  group_by(officer_defined_ethnicity) %>%
  summarise(total = sum(count))

combined_data %>%
  # Convert datetime to Date and extract year
  mutate(date = as.Date(ymd_hms(datetime)), year = year(date)) %>% 
  ungroup() %>%
  # Filter out rows with NA in officer_defined_ethnicity
  filter(!is.na(officer_defined_ethnicity)) %>%
  # Count occurrences by date and officer_defined_ethnicity
  count(date, officer_defined_ethnicity) %>%
  # Merge with census data to get weighted search rates
  merge(census_race, by = "officer_defined_ethnicity") %>%
  # Calculate standardized counts as per 100 of total population for each ethnicity
  mutate(n = n*100/total) %>%
  # Plot the data using a line graph
  ggplot(aes(x = date, y = n, color = officer_defined_ethnicity)) +
    geom_line() + # Line graph to show trends over time
    scale_x_date(
      breaks = date_breaks("3 months"), # X-axis breaks every 3 months
      labels = date_format("%b %Y") # Format X-axis labels
    ) +
    labs(
      title = "Stop and Searches Rate per Day by Ethnicity (2020-2023)",
      x = "Date",
      y = "Stop and Searches Rates (%)",
      color = "Ethnicity"
    ) +
    theme_minimal() + # Use a minimal theme for clarity
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Adjust X-axis text for readability

```

White individuals have the lowest and most consistent daily stop-and-search rates, while Black individuals face significantly higher rates, with spikes indicating periods of increased police activity.

#### Age Range

```{r Age, echo=FALSE}
# Reshape census data and calculate demographic ratios in stop and searches by age group over time
census_age <- gather(census, key = "race", value = "count", -age_range) # Reshape census data

# Group by age_range and calculate total count for each age group
census_age <- census_age %>%
  group_by(age_range) %>%
  summarise(total = sum(count))

combined_data %>%
  # Convert datetime to Date and extract year
  mutate(date = as.Date(ymd_hms(datetime)), year = year(date)) %>%
  ungroup() %>%
  # Count occurrences by date and age_range
  count(date, age_range) %>%
  # Merge with census data to get weighted search rates by age group
  merge(census_age, by = "age_range") %>%
  # Calculate standardized counts as per 100 of total population for each age group
  mutate(n = n*100/total) %>%
  # Plot the data using a line graph
  ggplot(aes(x = date, y = n, color = age_range)) +
    geom_line() + # Line graph to show trends over time
    scale_x_date(
      breaks = date_breaks("3 months"), # X-axis breaks every 3 months
      labels = date_format("%b %Y") # Format X-axis labels
    ) +
    labs(
      title = "Stop and Searches per Day by Age Group (2020-2023)",
      x = "Date",
      y = "Stop and Searches Rates (%)",
      color = "age group" # Label for the color legend
    ) +
    theme_minimal() + # Use a minimal theme for clarity
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Adjust X-axis text for readability

```

The 25-34 age group experiences the highest rate of stop-and-searches with daily variations, while those over 34 have a lower rate.

#### Gender

```{r Gender, echo=FALSE}
combined_data %>%
  # Convert 'datetime' to Date format and extract the year
  mutate(date = as.Date(ymd_hms(datetime)), year = year(date)) %>% 
  # Filter out rows where 'gender' is NA
  filter(!is.na(gender)) %>%
  ungroup() %>%
  # Count occurrences by date and gender
  count(date, gender) %>%
  # Plot the data using a line graph
  ggplot(aes(x = date, y = n, color = gender)) +
    geom_line() + # Line graph to show trends over time by gender
    scale_x_date(
      breaks = date_breaks("3 months"), # X-axis breaks every 3 months
      labels = date_format("%b %Y") # Format X-axis labels as month and year
    ) +
    labs(
      title = "Stop and Searches per Day by Gender (2020-2023)",
      x = "Date",
      y = "Number of Stop and Searches", # Y-axis label
      color = "Gender" # Label for the color legend
    ) +
    theme_minimal() + # Use a minimal theme for clarity
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Adjust X-axis text for readability
```

males experiencing the highest daily rates. Females and 'Other' genders had far fewer searches, indicating a significant gender disparity.

From October 2022 to January 2023 and April to July 2023, race impacted police stop-and-search rates more than age or gender, suggesting racial factors predominantly influence police searches.

### Measurement of Biases

Bias in police stop-and-search is defined by differing outcomes among groups. 'Hit rate' indicates the likelihood of outcomes more serious than 'no action' or 'simple caution,' and 'arrest rate' reflects the probability of arrest. Each bubble in our analysis represents the search count for a police authority.

```{r arrest&hit, echo=FALSE}
# Calculate arrest rates and hit rates for different ethnicities, then visualize

# Part 1: Arrest Rate Calculation
# Create a binary outcome for arrest (1 if arrested, 0 otherwise)
arrestRate_master <- combined_data %>% 
  mutate(outcome_binary = case_when(
    outcome == "Arrest" ~ 1,
    TRUE ~ 0
  )) %>%
  group_by(officer_defined_ethnicity, force) %>%
  summarise(
    arrest_Rate = mean(outcome_binary, na.rm = TRUE), # Calculate average arrest rate
    .groups = 'drop'
  ) %>%
  mutate(arrest_Rate = ifelse(is.na(arrest_Rate), 0, arrest_Rate)) # Handle NAs

# Part 2: Total Search Calculation
# Count the total searches by ethnicity and force
totSearch_master <- combined_data %>%
  group_by(officer_defined_ethnicity, force) %>%
  summarise(
    searches = n(), # Count total searches
    .groups = 'drop'
  ) %>%
  mutate(searches = ifelse(is.na(searches), 0, searches)) # Handle NAs

# Reshape and merge arrest rate data
arrestRate_master <- arrestRate_master %>% 
  filter(officer_defined_ethnicity %in% c("Asian", "Black", "Other", "White")) %>% 
  spread(officer_defined_ethnicity, arrest_Rate, fill = 0) %>% 
  rename(white_arrest_rate = White) %>% 
  gather(minority_race, minority_arrest_rate, c(Asian, Black, Other)) %>%
  arrange(force)

# Reshape and total search data
totSearch_master <- totSearch_master %>% 
  spread(officer_defined_ethnicity, searches, fill = 0) %>%
  rename(white_searches = White) %>%
  gather(minority_race, minority_searches, c(Asian, Black, Other)) %>%
  mutate(num_searches = minority_searches + white_searches) 

# Merge and plot arrest rate data
final_arrest_master <- merge(arrestRate_master, totSearch_master, by = c("force", "minority_race"))
arrest_rate_columns <- grep("arrest_rate$", names(arrestRate_master), value = TRUE)
max_arrest_rate <- max(sapply(arrestRate_master[arrest_rate_columns], max, na.rm = TRUE))

# Arrest Rate Visualization Setup
arrest <- ggplot(final_arrest_master, aes(x = white_arrest_rate, y = minority_arrest_rate)) +
  geom_point(aes(size = num_searches), pch = 21) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_x_continuous("White arrest rate", limits = c(0, max_arrest_rate + 0.01), labels = scales::percent) +
  scale_y_continuous("Minority arrest rate", limits = c(0, max_arrest_rate + 0.01), labels = scales::percent) +
  scale_size(name = "Number of Searches", breaks = c(10000, 20000, 30000), labels = c("10k searches", "20k searches", "30k searches")) +
  coord_fixed() +
  theme_light() +
  facet_grid(. ~ minority_race)

# Part 3: Hit Rate Calculation
# Create a binary outcome for hit rate
hitRate_master <- combined_data %>% 
  mutate(outcome_binary = case_when(
    outcome == "Caution (simple or conditional)" ~ 0,
    outcome == "A no further action disposal" ~ 0,
    TRUE ~ 1
  )) %>%
  group_by(officer_defined_ethnicity, force) %>%
  summarise(
    hit_rate = mean(outcome_binary, na.rm = TRUE), # Calculate average hit rate
    .groups = 'drop'
  ) %>%
  mutate(hit_rate = ifelse(is.na(hit_rate), 0, hit_rate)) # Handle NAs

# Reshape and merge hit rate data
hitRate_master <- hitRate_master %>% 
  filter(officer_defined_ethnicity %in% c("Asian", "Black", "Other", "White")) %>% 
  spread(officer_defined_ethnicity, hit_rate, fill = 0) %>% 
  rename(white_hit_rate = White) %>% 
  gather(minority_race, minority_hit_rate, c(Asian, Black, Other)) %>%
  arrange(force)

# Merge and plot hit rate data
final_hit_master <- merge(hitRate_master, totSearch_master, by = c("force", "minority_race"))
hit_rate_columns <- grep("hit_rate$", names(hitRate_master), value = TRUE)
max_hit_rate <- max(sapply(hitRate_master[hit_rate_columns], max, na.rm = TRUE))

# Hit Rate Visualization Setup
hit <- ggplot(final_hit_master, aes(x = white_hit_rate, y = minority_hit_rate)) +
  geom_point(aes(size = num_searches), pch = 21) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_x_continuous("White hit rate", limits = c(0, max_hit_rate + 0.01), labels = scales::percent) +
  scale_y_continuous("Minority hit rate", limits = c(0, max_hit_rate + 0.01), labels = scales::percent) +
  scale_size(name = "Number of Searches", breaks = c(10000, 20000, 30000), labels = c("10k searches", "20k searches", "30k searches")) +
  coord_fixed() +
  theme_light() +
  facet_grid(. ~ minority_race)

# Combine and display the plots
grid.arrange(hit, arrest, ncol = 1)


```

Hit rate analysis reveals that Asian and Black minorities encounter higher search consequences than Whites, and they are also arrested more often, indicating potential stop-and-search bias. Geographic disparities are quantified by an index comparing search log-odds for Black versus White individuals; values above zero indicate a higher rate for Blacks, below zero for Whites, and zero denotes parity.

```{r, echo=FALSE}

# Visualize racial profiling in searches by location using a logarithmic index

# Step 1: Grouping and Summarizing Search Data
combined_data %>%
  group_by(officer_defined_ethnicity, force) %>% # Group by officer ethnicity and force
  summarise(searches = n(), .groups = "drop") %>% # Summarize total searches and drop grouping

# Step 2: Merging with Census Data
  merge(census_race, by = "officer_defined_ethnicity") %>%
  mutate(searches = searches*100/total) %>% # Calculate the percentage of searches per ethnicity
  select(!total) %>%

# Step 3: Reshaping Data for Visualization
  spread(officer_defined_ethnicity, searches, fill = 0) %>% # Spread data into wide format
  select("force", "Black", "White") %>%
  mutate(index = log(Black/White)) %>% # Calculate logarithmic index of Black to White search ratio

# Step 4: Preparing Data for Plotting
  select(force, index) %>% 
  mutate(force = fct_reorder(force, index)) %>%  # Reorder factor levels based on index

# Step 5: Creating the Bar Plot
  ggplot(aes(x = force, y = index)) +
    geom_bar(stat = "identity") + # Create bars for each force based on the index
    geom_hline(yintercept = 0, colour = "red", linetype = "dashed") + # Add a dashed line at y = 0 for reference

# Step 6: Adding Labels and Styling
    labs(
      title = "Racial Profiling in Searches by Location (2020-2023)"
    ) +
    theme_minimal() + # Minimalistic theme for clarity
    coord_flip() # Flip the coordinates to have categories on the y-axis for better readability

  
```

City Scale and Income Levels impact stop-and-search rates; larger cities like London, with denser populations, tend to have more, as do lower-income areas like Nottinghamshire, potentially due to higher crime rates.

### Generalized Linear Model

We fit a Generalized Linear Model to the search rate data to identify key factors of biased searches.

```{r, message=FALSE, include=FALSE, warning=FALSE}
# Prepare data for regression analysis with contrast coding and factor adjustments

# Step 1: Converting to Factors
combined_data$officer_defined_ethnicity <- as.factor(combined_data$officer_defined_ethnicity) # Convert officer ethnicity to factor
combined_data$force <- as.factor(combined_data$force) # Convert force to factor

# Step 2: Setting Reference Level
# Set 'White' as the reference level for officer_defined_ethnicity
combined_data$officer_defined_ethnicity <- relevel(combined_data$officer_defined_ethnicity, ref = "White")

# Step 3: Applying Contrast Coding
# Apply treatment contrast coding with 'White' as the base level
contrasts(combined_data$officer_defined_ethnicity) <- contr.treatment(levels(combined_data$officer_defined_ethnicity), base = 1)

# Step 4: Preparing Regression Data
# Filter out NA values in officer_defined_ethnicity and age_range
Reg_data <- combined_data %>% 
  filter(!is.na(officer_defined_ethnicity) & !is.na(age_range)) %>%
  group_by(officer_defined_ethnicity, force, age_range) %>%
  summarise(
    stops_searches = n(), # Count of stops and searches
    arrests = sum(outcome == "Arrest", na.rm = TRUE), # Count of arrests
    .groups = 'drop'
  ) %>%
  merge(census_race, by = "officer_defined_ethnicity") %>%
  mutate(test = stops_searches*1000/total) # Calculate stops and searches per 1000 individuals in each ethnicity
```

$$
\begin{align*}
S_{kji} &\sim \text{GLM}(\mu_{kji}) \\
\log(\mu_{kji}) &= \alpha + X_{1j} + X_{2k} + X_{3i} \\
X_{1j} &\sim \text{Categorical}(\text{Ethnicity}) \\
X_{2k} &\sim \text{Categorical}(\text{Police Force}) \\
X_{3i} &\sim \text{Categorical}(\text{Age Range})
\end{align*}
$$

Where:

-   $S_{k, j, i}$ is the rate of stop and searches in the $k$th police forces $(k = 1,...,39)$ of individuals of $j$th ethnicity $(j = 1,…,4)$ of $i$ age range $(i = 1,...,4)$

-   $\mu_{kji}$ is the expected count of stops and searches, modeled as a function of ethnicity, police force, and age range.

-   $α$ is a overall intercept

-   $X_{1j}$ *is an ethnicity intercept with* $μ_{4}$ = 0, therefore $μ_{j}$ is the difference from the white ethnic group

-   $X_{2k}$ is a *effecr* for $k$th police forces

-   $X_{3i}$ is a effect of $i$ age range

```{r, echo=FALSE}
# Fit a Generalized Linear Model (GLM)

# Creating the GLM model
glmm_model <- glm(
  test ~ officer_defined_ethnicity + age_range + force, # Model formula including fixed effects
  data = Reg_data # Dataset used for the model
)

# Display the summary of the model
summary(glmm_model)


```

The analysis shows that Black individuals are searched at a significantly higher rate than Whites (*b* = 0.40, *P* \< 0.001), indicating a bias. In the Metropolitan police force region, the search rate is notably higher (*b* = 4.44, *P* \< 0.001), especially in London.

### Decoding Biases: the **Veil of Darkness test**

To establish a causal relationship between race and stop-and-search decisions, we propose a "**Veil of Darkness**" (VOD) test based on Grogger and Ridgeway's 2006 concept, aims to establish a causal link between race and police stop-and-search decisions. It posits that racial profiling is less likely in darkness, as identifying race becomes more challenging. A lower proportion of stops involving black individuals at night compared to daytime could indicate racial profiling during daylight.

```{r, echo=FALSE}
# Convert datetime to just the hour component
combined_data$time <- format(as.POSIXct(combined_data$datetime, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC"), "%H")

# Count occurrences by time and ethnicity
combined_data %>%
  count(time, officer_defined_ethnicity) %>%
  merge(census_race, by = "officer_defined_ethnicity") %>%
  # Calculate rates per ethnicity and convert time to POSIXct format
  mutate(n = n*100 /total, time = as.POSIXct(time, format = "%H")) %>%
  # Create a line plot
  ggplot(aes(x = time, y = n, color = officer_defined_ethnicity)) +
    geom_line() +
    # Highlight night time periods with grey background
    annotate("rect", xmin = as.POSIXct("00:00", format = "%H"), xmax = as.POSIXct("06:00", format = "%H"), ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "grey") +
    annotate("rect", xmin = as.POSIXct("16:00", format = "%H"), xmax = as.POSIXct("23:00", format = "%H"), ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "grey") +
    # Add text annotations for day and night
    annotate("text", x = as.POSIXct("2:00", format = "%H"), y = Inf, label = "Night", vjust = 2, hjust = 0.5) +
    annotate("text", x = as.POSIXct("21:00", format = "%H"), y = Inf, label = "Night", vjust = 2, hjust = 0.5) +
    annotate("text", x = as.POSIXct("12:00", format = "%H"), y = Inf, label = "Day", vjust = 2, hjust = 0.5) +
    # Set x-axis breaks and labels
    scale_x_datetime(
      breaks = as.POSIXct(c("02:00", "04:00", "06:00", "08:00", "10:00", "12:00", "14:00", "16:00", "18:00", "20:00", "22:00"), format = "%H"),
      labels = date_format("%H")
    ) +
    # Labeling the plot
    labs(
      title = "Stop and Searches Rate Over Three Years by Ethnicity",
      x = "Time (Hourly)",
      y = "Stop and Searches Rates (%)",
      color = "Ethnicity"
    ) +
    # Minimal theme and adjusting x-axis text angle
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

-   higher stop and search rates during daylight hours, particularly for Black individuals.

-   The disparity between day and night rates may imply potential bias in stop and search practices based on visibility of race.

The comparison of stops during day and night is complicated by factors like enforcement strategies and societal behavior, which vary with time. To control for these, we compare stops during the "inter-twilight period," adjusting for seasonal light changes. In England and Wales, nightfall is at 4pm in winter and 9pm in summer. Focusing on Black and White individuals' searches sharpens our analysis. After filtering, we apply logistic regression to the refined dataset to examine stop-and-search frequency.

```{r, echo=FALSE}
# Subset combined_data to keep specific columns for analysis
vod_stops <- combined_data %>%
  select(datetime, officer_defined_ethnicity, force) %>%  # Selecting only datetime, ethnicity, and force columns
  
  # Filter for rows where ethnicity is either Black or White
  filter(officer_defined_ethnicity %in% c("Black", "White")) %>%
  # Creating new variables
  mutate(
    month = month(as.Date(substr(datetime, 1, 10))), # Extracting month from datetime
    time = hour(hms(substr(datetime, 12, 19))), # Extracting hour from datetime
    is_dark = ifelse(month %in% c(1, 2, 12), TRUE, FALSE), # Flag for dark months (Jan, Feb, Dec)
    is_black = ifelse(officer_defined_ethnicity == "Black", TRUE, FALSE) # Flag for Black ethnicity
  ) %>%
  # Filter for rows in specified months (Jan, Feb, Jun, Jul, Aug, Dec)
  filter(month %in% c(1, 2, 6, 7, 8, 12)) %>%
  # Filter for rows within the specified time range (4 PM to 9 PM)
  filter(time %in% 16:21) %>%
  # Merge with census_race data based on officer_defined_ethnicity
  merge(census_race, by = "officer_defined_ethnicity")

# Count the number of rows in the filtered dataset
vod_stops %>% nrow()

```

$$
\begin{equation}
\Pr(\text{black} | t, j, p) = \text{logit}^{-1} \left( \beta_0 + \beta_1 \times p + ns_6(t) + \sum_{j} \gamma_j \times I(\text{district} = j) \right)
\end{equation}
$$

-   $\Pr(\text{black} | t, g, p)$ denotes the probability of the event that **`is_black`** equals 1 given the predictors at time of $t$, by police force $j$ and season $p$.

-   $\beta_0$ is the intercept.

-   $\beta_1$ is the coefficient for the binary predictor **`is_dark`**.

-   $ns_6(t)$ represents the natural spline $t$ for with 6 degrees of freedom.

-   $γ_j$ are the coefficients for the factor levels of **`force`**, with the indicator function $I$ being 1 if the condition is true (if the observation is in force $j$) and 0 otherwise.

```{r}
# Fit a Generalized Linear Model (GLM) using binomial family
vod <- glm(
  is_black ~ is_dark + splines::ns(time, df = 6) + as.factor(force), # Model formula
  family = binomial, # Binomial family, appropriate for binary outcomes
  data = vod_stops # Data source
)

# Extract and display the estimate and standard error for 'is_dark' (TRUE level) from the model summary
summary(vod)$coefficients["is_darkTRUE", c("Estimate", "Std. Error")]

```

A negative coefficient indicates that the cover of darkness reduces the probability of Black individuals being stopped. The small standard error around this coefficient reinforces its statistical significance, confirming the robustness of the finding. let see this effect in figures:

```{r, echo=FALSE}
# Summer Plot
summer <- vod_stops %>% 
  # Filter for records during dark months (representing summer)
  filter(is_dark = TRUE) %>%
  # Count occurrences by ethnicity and time
  count(officer_defined_ethnicity, time) %>% 
  # Merge with census race data
  merge(census_race, by = "officer_defined_ethnicity") %>%
  # Calculate rates per ethnicity
  mutate(n = n*100 /total) %>%
  # Create a line plot
  ggplot(aes(x = time, y = n, color = officer_defined_ethnicity)) +
    geom_line() +
    # Set y-axis limits
    scale_y_continuous(limits = c(0.0, 0.6)) +
    # Add labels and titles
    labs(
      title = "Summer",
      x = "Time (Hourly)",
      y = "Number of Stop and Searches",
      color = "Ethnicity"
    ) +
    # Add text annotation for "Day"
    annotate("text", x = 18, y = Inf, label = "Day", vjust = 2, hjust = 0.5) +
    # Use minimal theme and adjust x-axis text angle
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Winter Plot
winter <- vod_stops %>% 
  # Filter for records during light months (representing winter)
  filter(is_dark == "FALSE") %>%
  # Count occurrences by ethnicity and time
  count(officer_defined_ethnicity, time) %>% 
  # Merge with census race data
  merge(census_race, by = "officer_defined_ethnicity") %>%
  # Calculate rates per ethnicity
  mutate(n = n*100 /total) %>%
  # Create a line plot
  ggplot(aes(x = time, y = n, color = officer_defined_ethnicity)) +
    geom_line() +
    # Set y-axis limits
    scale_y_continuous(limits = c(0.0, 0.6)) +
    # Add labels and titles
    labs(
      title = "Winter",
      x = "Time (Hourly)",
      y = "Stop and Searches Rates (%)",
      color = "Ethnicity"
    ) +
    # Highlight evening hours with grey background
    annotate("rect", xmin = 16, xmax = 21, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "grey") +
    # Use minimal theme and adjust x-axis text angle
    theme_minimal() +
    # Add text annotation for "Night"
    annotate("text", x = 18, y = Inf, label = "Night", vjust = 2, hjust = 0.5) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Arrange the winter and summer plots side by side
grid.arrange(winter, summer, ncol = 2)
```

Daytime stop-and-search disparities for Black individuals imply racial profiling, which seems to decrease at night, aligning with the VOD hypothesis. This necessitates a closer examination of police practices during varying light conditions.

## References:

Grogger, J., & Ridgeway, G. (2006). Testing for racial profiling in traffic stops from behind a veil of darkness. *Journal of the American Statistical Association*, *101*(475), 878-887.

## Appendix Code

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
